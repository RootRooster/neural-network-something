{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "778b1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pydicom\n",
    "import os \n",
    "import numpy as np\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery',\n",
    "    'Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery',\n",
    "    'Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery',\n",
    "    'Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery',\n",
    "    'Left Anterior Cerebral Artery',\n",
    "    'Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery',\n",
    "    'Right Posterior Communicating Artery',\n",
    "    'Basilar Tip',\n",
    "    'Other Posterior Circulation',\n",
    "    'Aneurysm Present',\n",
    "]\n",
    "WORK_DIR = Path(\"./\")\n",
    "DATA_DIR = Path(\"./data\")\n",
    "SERIES_DIR = Path(\"./series\")\n",
    "PROCESSING_DIR = Path (\"./processing_csvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b46fa860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in dataset sizes:\n",
      "2251 4348 4402\n",
      "2249 4346 4400\n"
     ]
    }
   ],
   "source": [
    "df_localizers = pd.read_csv(PROCESSING_DIR / \"train_localizers_proc.csv\")\n",
    "df_train = pd.read_csv(PROCESSING_DIR / \"train_proc.csv\")\n",
    "df_dimensions = pd.read_csv(PROCESSING_DIR / \"dimensions_proc.csv\")\n",
    "\n",
    "print(\"Change in dataset sizes:\")\n",
    "print(len(df_localizers), len(df_train), len(df_dimensions))\n",
    "# There are no such files in the provided documents\n",
    "to_remove = [\n",
    "    #SeriesInstanceUID                                                   #SOPInstanceUID #\n",
    "    (\"1.2.826.0.1.3680043.8.498.11145695452143851764832708867797988068\", \"1.2.826.0.1.3680043.8.498.11359680660692538603323710088085312565\"),\n",
    "    (\"1.2.826.0.1.3680043.8.498.35204126697881966597435252550544407444\", \"1.2.826.0.1.3680043.8.498.50473067775982707701946022117324201859\")\n",
    "]\n",
    "\n",
    "# this step removes some data that cant be found\n",
    "for series_uid, sop_uid in to_remove:\n",
    "    df_localizers = df_localizers[(df_localizers[\"SeriesInstanceUID\"] != series_uid) | (df_localizers[\"SOPInstanceUID\"] != sop_uid)]\n",
    "    df_train = df_train[(df_train[\"SeriesInstanceUID\"] != series_uid)]\n",
    "    df_dimensions = df_dimensions[(df_dimensions[\"SeriesInstanceUID\"] != series_uid)]\n",
    "\n",
    "df_joined = pd.merge(df_train, df_localizers, on='SeriesInstanceUID', how='left')\n",
    "df_ultimate = pd.merge(df_joined, df_dimensions, on=\"SeriesInstanceUID\", how=\"left\")\n",
    "print(len(df_localizers), len(df_train), len(df_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecf6e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df_ultimate[df_ultimate['Shape'].str.count(',') == 2]\n",
    "# test_set.groupby(\"Depth\").count() # -- All have depth of 1 so just one dicom file\n",
    "# test_set[test_set.duplicated(subset='SeriesInstanceUID', keep=False)] #\n",
    "# test_set[test_set.duplicated(subset=['SeriesInstanceUID', 'SOPInstanceUID'] , keep=False)] # has duplicates for differnet locations and coordinats\n",
    "test_set = test_set.drop_duplicates(subset='SeriesInstanceUID')\n",
    "\n",
    "# SINGLE THREAD\n",
    "def process_dicom_files(series_root_path, output_path):\n",
    "    test_folder = os.path.join(output_path, 'test')\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "    \n",
    "    for series_instance_uid in test_set['SeriesInstanceUID']:\n",
    "        series_folder_path = os.path.join(series_root_path, str(series_instance_uid))\n",
    "        if not os.path.exists(series_folder_path):\n",
    "            print(f\"Series folder for UID {series_instance_uid} does not exist.\")\n",
    "            continue\n",
    "        dcm_files = [f for f in os.listdir(series_folder_path) if f.endswith('.dcm')]\n",
    "        if len(dcm_files) != 1:\n",
    "            print(f\"Series UID {series_instance_uid} has {len(dcm_files)} DICOM files instead of one.\")\n",
    "            continue\n",
    "        dcm_file_path = os.path.join(series_folder_path, dcm_files[0])\n",
    "        dcm_data = pydicom.dcmread(dcm_file_path)\n",
    "        pixel_array = dcm_data.pixel_array\n",
    "        output_series_folder = os.path.join(test_folder, str(series_instance_uid))\n",
    "        os.makedirs(output_series_folder, exist_ok=True)\n",
    "        output_file_path = os.path.join(output_series_folder, dcm_files[0].replace('.dcm', '.npy'))\n",
    "        np.save(output_file_path, pixel_array)\n",
    "        print(f\"Processed series UID {series_instance_uid}, saved pixel data to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35756c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_single_test_series(series_instance_uid, series_root_path, output_path):\n",
    "    \"\"\"Process a single test series\"\"\"\n",
    "    series_folder_path = os.path.join(series_root_path, str(series_instance_uid))\n",
    "    \n",
    "    if not os.path.exists(series_folder_path):\n",
    "        return f\"Series folder for UID {series_instance_uid} does not exist.\\n\"\n",
    "    \n",
    "    dcm_files = [f for f in os.listdir(series_folder_path) if f.endswith('.dcm')]\n",
    "    \n",
    "    if len(dcm_files) != 1:\n",
    "        return f\"Series UID {series_instance_uid} has {len(dcm_files)} DICOM files instead of one.\\n\"\n",
    "    \n",
    "    # Process the single DICOM file\n",
    "    dcm_file_path = os.path.join(series_folder_path, dcm_files[0])\n",
    "    dcm_data = pydicom.dcmread(dcm_file_path)\n",
    "    pixel_array = dcm_data.pixel_array\n",
    "    \n",
    "    # Save to output folder (unique per series - no race conditions)\n",
    "    test_folder = os.path.join(output_path, 'test')\n",
    "    output_series_folder = os.path.join(test_folder, str(series_instance_uid))\n",
    "    os.makedirs(output_series_folder, exist_ok=True)\n",
    "    \n",
    "    output_file_path = os.path.join(output_series_folder, dcm_files[0].replace('.dcm', '.npy'))\n",
    "    np.save(output_file_path, pixel_array)\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def process_dicom_files_parallel(series_root_path, output_path, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Process DICOM files in parallel using joblib\n",
    "    n_jobs: number of parallel jobs (-1 uses all available cores)\n",
    "    \"\"\"\n",
    "    # Create test folder\n",
    "    test_folder = os.path.join(output_path, 'test')\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "    \n",
    "    # Prepare tasks\n",
    "    series_ids = test_set['SeriesInstanceUID'].tolist()\n",
    "    print(f\"Processing {len(series_ids)} series...\")\n",
    "    \n",
    "    # Process in parallel\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "        delayed(process_single_test_series)(series_id, series_root_path, output_path)\n",
    "        for series_id in series_ids\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    for result in results:\n",
    "        if result:  # Only print non-empty results\n",
    "            print(result, end=\"\")\n",
    "    \n",
    "    print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3cb95ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT IF YOU WANT TO RELOAD THE DATA\n",
    "# process_dicom_files_parallel(str(SERIES_DIR), str(DATA_DIR))\n",
    "def find_sop_instance_id(series_instance_id, base_dir):\n",
    "    series_path = os.path.join(base_dir, series_instance_id)\n",
    "    if not os.path.exists(series_path):\n",
    "        return None\n",
    "    files = os.listdir(series_path)\n",
    "    if files:\n",
    "        return files[0].rstrip('.npy') \n",
    "    return None\n",
    "\n",
    "def update_dataframe_with_sop_instance_id(df, base_dir):\n",
    "    # Apply the function to find SOPInstanceID for each row\n",
    "    df['SOPInstanceUID'] = df['SeriesInstanceUID'].apply(find_sop_instance_id, base_dir=base_dir)\n",
    "    return df\n",
    "\n",
    "# Update the DataFrame\n",
    "df = update_dataframe_with_sop_instance_id(test_set, './data/test')\n",
    "df.to_csv('./test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9b2b0eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SeriesInstanceUID</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.8.498.10005158603912009425635473100344077317</th>\n",
       "      <th>1.2.826.0.1.3680043.8.498.10775329348174902199350466348663848346</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.8.498.10022796280698534221758473208024838831</th>\n",
       "      <th>1.2.826.0.1.3680043.8.498.53868409774237283281776807176852774246</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.8.498.10023411164590664678534044036963716636</th>\n",
       "      <th>1.2.826.0.1.3680043.8.498.24186535344744886473554579401056227253</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.8.498.10030095840917973694487307992374923817</th>\n",
       "      <th>1.2.826.0.1.3680043.8.498.75217084841854214544099244823406151875</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.8.498.10034081836061566510187499603024895557</th>\n",
       "      <th>1.2.826.0.1.3680043.8.498.71237104731452368587327801789352569583</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.2.826.0.1.3680043.8.498.99887675554378211308175946117895608384</th>\n",
       "      <th>1.2.826.0.1.3680043.8.498.10885430363476327277192154022897733247</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.8.498.75978746530527925899354153686225196613</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.2.826.0.1.3680043.8.498.99892390884723813599532075083872271516</th>\n",
       "      <th>1.2.826.0.1.3680043.8.498.12398549862508001109149426855485142650</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.8.498.21598979799967012280125410147654260304</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.8.498.99985209798463601651869595532975221005</th>\n",
       "      <th>1.2.826.0.1.3680043.8.498.65157374409757364881520821698981129559</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2069 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       coordinates  \\\n",
       "SeriesInstanceUID                                  SOPInstanceUID                                                    \n",
       "1.2.826.0.1.3680043.8.498.100051586039120094256... 1.2.826.0.1.3680043.8.498.107753293481749021993...            1   \n",
       "1.2.826.0.1.3680043.8.498.100227962806985342217... 1.2.826.0.1.3680043.8.498.538684097742372832817...            1   \n",
       "1.2.826.0.1.3680043.8.498.100234111645906646785... 1.2.826.0.1.3680043.8.498.241865353447448864735...            1   \n",
       "1.2.826.0.1.3680043.8.498.100300958409179736944... 1.2.826.0.1.3680043.8.498.752170848418542145440...            1   \n",
       "1.2.826.0.1.3680043.8.498.100340818360615665101... 1.2.826.0.1.3680043.8.498.712371047314523685873...            1   \n",
       "...                                                                                                            ...   \n",
       "1.2.826.0.1.3680043.8.498.998876755543782113081... 1.2.826.0.1.3680043.8.498.108854303634763272771...            1   \n",
       "                                                   1.2.826.0.1.3680043.8.498.759787465305279258993...            1   \n",
       "1.2.826.0.1.3680043.8.498.998923908847238135995... 1.2.826.0.1.3680043.8.498.123985498625080011091...            2   \n",
       "                                                   1.2.826.0.1.3680043.8.498.215989797999670122801...            2   \n",
       "1.2.826.0.1.3680043.8.498.999852097984636016518... 1.2.826.0.1.3680043.8.498.651573744097573648815...            1   \n",
       "\n",
       "                                                                                                       location  \n",
       "SeriesInstanceUID                                  SOPInstanceUID                                                \n",
       "1.2.826.0.1.3680043.8.498.100051586039120094256... 1.2.826.0.1.3680043.8.498.107753293481749021993...         1  \n",
       "1.2.826.0.1.3680043.8.498.100227962806985342217... 1.2.826.0.1.3680043.8.498.538684097742372832817...         1  \n",
       "1.2.826.0.1.3680043.8.498.100234111645906646785... 1.2.826.0.1.3680043.8.498.241865353447448864735...         1  \n",
       "1.2.826.0.1.3680043.8.498.100300958409179736944... 1.2.826.0.1.3680043.8.498.752170848418542145440...         1  \n",
       "1.2.826.0.1.3680043.8.498.100340818360615665101... 1.2.826.0.1.3680043.8.498.712371047314523685873...         1  \n",
       "...                                                                                                         ...  \n",
       "1.2.826.0.1.3680043.8.498.998876755543782113081... 1.2.826.0.1.3680043.8.498.108854303634763272771...         1  \n",
       "                                                   1.2.826.0.1.3680043.8.498.759787465305279258993...         1  \n",
       "1.2.826.0.1.3680043.8.498.998923908847238135995... 1.2.826.0.1.3680043.8.498.123985498625080011091...         2  \n",
       "                                                   1.2.826.0.1.3680043.8.498.215989797999670122801...         2  \n",
       "1.2.826.0.1.3680043.8.498.999852097984636016518... 1.2.826.0.1.3680043.8.498.651573744097573648815...         1  \n",
       "\n",
       "[2069 rows x 2 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive set v1\n",
    "positive_set = df_localizers\n",
    "positive_set[positive_set.duplicated(subset=['SeriesInstanceUID'], keep=False)]\n",
    "test_set_ids = set(test_set['SeriesInstanceUID'])\n",
    "positive_set = positive_set[~positive_set['SeriesInstanceUID'].isin(test_set_ids)]\n",
    "#df_test_positive_set = df_ultimate[df_ultimate['Shape'].str.count(',') == 2]\n",
    "positive_set_ids = set(positive_set['SeriesInstanceUID'])\n",
    "#df_test_positive_set[df_test_positive_set['SeriesInstanceUID'].isin(positive_set_ids)]\n",
    "positive_set.groupby([\"SeriesInstanceUID\",\"SOPInstanceUID\"]).count() # 2099 positive slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c2df29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_single_positive_series(series_instance_uid, series_root_path, output_folder, df_ultimate):\n",
    "    \"\"\"Process a single positive series\"\"\"\n",
    "    series_folder_path = os.path.join(series_root_path, str(series_instance_uid))\n",
    "    \n",
    "    results = {\n",
    "        'positions_sampled': [],\n",
    "        'validation_train_split': [],\n",
    "        'processed_count': 0,\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(series_folder_path):\n",
    "        results['warnings'].append(f\"Series folder for UID {series_instance_uid} does not exist.\")\n",
    "        return results\n",
    "    \n",
    "    sop_instances = set(df_ultimate[df_ultimate[\"SeriesInstanceUID\"]==series_instance_uid][\"SOPInstanceUID\"])\n",
    "    \n",
    "    dcm_file_paths =[os.path.join(series_folder_path,f) for f in os.listdir(series_folder_path) if f.endswith('.dcm')]\n",
    "    \n",
    "    if len(dcm_file_paths) == 0:\n",
    "        results['warnings'].append(f\"No DICOM files found in series folder: {series_folder_path}\")\n",
    "        return results\n",
    "    \n",
    "    dcm_instance_numbers = []\n",
    "    for dcm in dcm_file_paths:\n",
    "        ds = pydicom.dcmread(dcm, stop_before_pixels=True)\n",
    "        if hasattr(ds, 'InstanceNumber'):\n",
    "            dcm_instance_numbers.append(ds.InstanceNumber)\n",
    "        else:\n",
    "            results['warnings'].append(f\"No instance number found in DICOM file: {dcm} in series {series_instance_uid}\")\n",
    "    if not dcm_instance_numbers:\n",
    "        results['warnings'].append(f\"No valid instance numbers found in DICOM files for series {series_instance_uid}\")\n",
    "        maximum = len(dcm_file_paths)  # Fallback if no instance numbers found\n",
    "        minimum = 0\n",
    "    else:\n",
    "        maximum = max(dcm_instance_numbers)\n",
    "        minimum = min(dcm_instance_numbers)\n",
    "\n",
    "    # Determine output type based on folder\n",
    "    output_type = 'val' if 'validation' in output_folder else 'train'\n",
    "    \n",
    "    for sop in sop_instances:\n",
    "        dcm_file_path = os.path.join(series_folder_path, f\"{sop}.dcm\")\n",
    "        if os.path.exists(dcm_file_path):\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_file_path)\n",
    "                pixel_array = dcm_data.pixel_array\n",
    "                \n",
    "                output_series_folder = os.path.join(output_folder, str(series_instance_uid))\n",
    "                os.makedirs(output_series_folder, exist_ok=True)\n",
    "                \n",
    "                output_file_path = os.path.join(output_series_folder, f\"{sop}.npy\")\n",
    "                instance_number = dcm_data.InstanceNumber\n",
    "                sample_position = (instance_number - minimum) / (maximum - minimum)\n",
    "                \n",
    "                np.save(output_file_path, pixel_array)\n",
    "                \n",
    "                results['positions_sampled'].append(sample_position)\n",
    "                results['validation_train_split'].append((series_instance_uid, sop, sample_position, output_type))\n",
    "                results['processed_count'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                results['warnings'].append(f\"Error processing {dcm_file_path}: {str(e)}\")\n",
    "        else:\n",
    "            results['warnings'].append(f\"No DICOM file found {dcm_file_path}.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_dicom_files_positives_parallel(series_root_path, output_path, positive_set_ids, df_ultimate, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Process DICOM files in parallel\n",
    "    \"\"\"\n",
    "    # Create folders\n",
    "    train_folder = os.path.join(output_path, 'train')\n",
    "    validation_folder = os.path.join(output_path, 'validation')\n",
    "    \n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(validation_folder, exist_ok=True)\n",
    "    \n",
    "    # Pre-determine train/validation split\n",
    "    total_series = len(positive_set_ids)\n",
    "    train_count = int(total_series * 0.8)\n",
    "    \n",
    "    l_positive_set_ids = list(positive_set_ids)\n",
    "    random.shuffle(l_positive_set_ids)  # Shuffle to ensure random split\n",
    "    train_series = l_positive_set_ids[:train_count]\n",
    "    val_series = l_positive_set_ids[train_count:]\n",
    "    \n",
    "    print(f\"Processing {len(train_series)} training series and {len(val_series)} validation series...\")\n",
    "    \n",
    "    # Prepare tasks\n",
    "    train_tasks = [(series_id, series_root_path, train_folder, df_ultimate) for series_id in train_series]\n",
    "    val_tasks = [(series_id, series_root_path, validation_folder, df_ultimate) for series_id in val_series]\n",
    "    \n",
    "    all_tasks = train_tasks + val_tasks\n",
    "    \n",
    "    # Process in parallel\n",
    "    print(\"Starting parallel processing...\")\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "        delayed(process_single_positive_series)(series_id, series_root_path, output_folder, df_ultimate)\n",
    "        for series_id, series_root_path, output_folder, df_ultimate in all_tasks\n",
    "    )\n",
    "    \n",
    "    # Combine results\n",
    "    validation_train_split = {'val': [], 'train': []}\n",
    "    positions_sampled = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    for result in results:\n",
    "        positions_sampled.extend(result['positions_sampled'])\n",
    "        total_processed += result['processed_count']\n",
    "        \n",
    "        # Add to appropriate split\n",
    "        for series_id, sop, pos, output_type in result['validation_train_split']:\n",
    "            validation_train_split[output_type].append((series_id, sop, pos))\n",
    "        \n",
    "        # Print warnings\n",
    "        for warning in result['warnings']:\n",
    "            warnings.warn(warning)\n",
    "    \n",
    "    print(f\"Processing complete! Processed {total_processed} files total.\")\n",
    "    print(f\"Train samples: {len(validation_train_split['train'])}\")\n",
    "    print(f\"Validation samples: {len(validation_train_split['val'])}\")\n",
    "    \n",
    "    return validation_train_split, positions_sampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b2ff94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datafames_for(array, df_localizers, df_train, skip_loc=False):\n",
    "    # Collect data in lists\n",
    "    class_data = []\n",
    "    locators_data = []\n",
    "    \n",
    "    def get_label_values(series_uid, sop_uid, df_localizers):\n",
    "        filtered_values = df_localizers[\n",
    "            (df_localizers['SeriesInstanceUID'] == series_uid) &\n",
    "            (df_localizers['SOPInstanceUID'] == sop_uid)\n",
    "        ]['location'].values\n",
    "        label_array = np.zeros(len(LABEL_COLS))\n",
    "        label_array[np.isin(LABEL_COLS, filtered_values)] = 1\n",
    "        if np.any(label_array[:-1]):\n",
    "            label_array[-1] = 1\n",
    "        return label_array\n",
    "\n",
    "    def create_locator_rows(series_uid, sop_uid, df_localizers, sample_position):\n",
    "        filtered_localizer = df_localizers[\n",
    "            (df_localizers['SeriesInstanceUID'] == series_uid) & \n",
    "            (df_localizers['SOPInstanceUID'] == sop_uid)\n",
    "        ]\n",
    "        coordinates_and_types = filtered_localizer[['coordinates', 'location']].values\n",
    "        rows = []\n",
    "        for coordinate, a_type in coordinates_and_types:\n",
    "            rows.append({\n",
    "                'SeriesInstanceUID': series_uid,\n",
    "                'SOPInstanceUID': sop_uid,\n",
    "                'Coordinates': coordinate,\n",
    "                'Coordinates Type': a_type,\n",
    "                'Sample position': sample_position\n",
    "            })\n",
    "        return rows\n",
    "\n",
    "    for series_uid, sop, sample_position in array:\n",
    "        # Collect classification data\n",
    "        label_array = get_label_values(series_uid, sop, df_localizers)\n",
    "        modality = df_train[\n",
    "            (df_train['SeriesInstanceUID'] == series_uid)\n",
    "        ]['Modality'].iloc[0]\n",
    "        \n",
    "        # Create dictionary for this row\n",
    "        class_row = {\n",
    "            'SeriesInstanceUID': series_uid,\n",
    "            'SOPInstanceUID': sop,  # Fixed: was sop_uid\n",
    "            'Modality': modality\n",
    "        }\n",
    "        # Add label columns\n",
    "        for i, label_col in enumerate(LABEL_COLS):\n",
    "            class_row[label_col] = label_array[i]\n",
    "\n",
    "        class_data.append(class_row)\n",
    "\n",
    "        # Collect locator data\n",
    "        if not skip_loc:\n",
    "            locator_rows = create_locator_rows(series_uid, sop, df_localizers, sample_position)\n",
    "            locators_data.extend(locator_rows)\n",
    "\n",
    "\n",
    "    # Create DataFrames once at the end\n",
    "    class_dataframe = pd.DataFrame(class_data)\n",
    "    locators_dataframe = pd.DataFrame(locators_data)\n",
    "    return class_dataframe, locators_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "efa4f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1376 training series and 344 validation series...\n",
      "Starting parallel processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 11 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikcadez/Masters local/1-year/2-semester/deep-learning/homeworks/final-project-v2/.venv/lib/python3.13/site-packages/pydicom/pixels/utils.py:222: UserWarning: A value of 'None' for (0028,0008) 'Number of Frames' is invalid, assuming 1 frame\n",
      "  warn_and_log(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Processed 2069 files total.\n",
      "Train samples: 1645\n",
      "Validation samples: 424\n"
     ]
    }
   ],
   "source": [
    "# uncomment the following to process dcom files and before delete the files in the train and validation set\n",
    "val_train_split, positions_sampled = process_dicom_files_positives_parallel(str(SERIES_DIR), str(DATA_DIR), positive_set_ids, df_ultimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b20efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array = val_train_split['train']\n",
    "val_array = val_train_split['val']\n",
    "df_training_class_positive, df_training_labels_positive = create_datafames_for(train_array, df_localizers, df_train)\n",
    "df_validation_class_positive, df_validation_labels_positive = create_datafames_for(val_array, df_localizers, df_train)\n",
    "# test the group by and count\n",
    "len(df_training_labels_positive.groupby(['SeriesInstanceUID', 'SOPInstanceUID']).count()) == len(df_training_class_positive.groupby(['SeriesInstanceUID', 'SOPInstanceUID']).count()) == len(df_training_class_positive)\n",
    "len(df_validation_labels_positive.groupby(['SeriesInstanceUID', 'SOPInstanceUID']).count()) == len(df_validation_class_positive.groupby(['SeriesInstanceUID', 'SOPInstanceUID']).count()) == len(df_validation_class_positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "44b6fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the negative samples\n",
    "negative_set = df_train[~df_train['SeriesInstanceUID'].isin(positive_set_ids)]\n",
    "negative_set = negative_set[~negative_set['SeriesInstanceUID'].isin(test_set_ids)]\n",
    "#test weather this worked:\n",
    "#negative_set_ids = set(negative_set['SeriesInstanceUID'])\n",
    "# positive_set[positive_set['SeriesInstanceUID'].isin(negative_set_ids)] -- nothing great!\n",
    "# test_set[test_set['SeriesInstanceUID'].isin(negative_set_ids)] # -- nothing great!\n",
    "#ns = set(negative_set[negative_set['Aneurysm Present']==1]['SeriesInstanceUID'])\n",
    "#df_localizers[df_localizers['SeriesInstanceUID'].isin(ns)]\n",
    "positives_in_negative_set = set(negative_set[negative_set['Aneurysm Present']==1]['SeriesInstanceUID'])# -- something....\n",
    "negative_set = negative_set[~negative_set['SeriesInstanceUID'].isin(positives_in_negative_set)]\n",
    "\n",
    "# Achive a more equal distribution between the negatives and the positives\n",
    "# print(negative_set['Modality'].value_counts()) # -- ['CT', 'MR'] -- so we can use the same code as above\n",
    "# Modality\n",
    "# CTA           860\n",
    "# MRA           622\n",
    "# MRI T2        622\n",
    "# MRI T1post    228\n",
    "# Name: count, dtype: int64\n",
    "# print(df_train[df_train['SeriesInstanceUID'].isin(positive_set_ids)]['Modality'].value_counts())\n",
    "# Modality\n",
    "# CTA           994\n",
    "# MRA           477\n",
    "# MRI T2        199\n",
    "# MRI T1post     78\n",
    "# Name: count, dtype: int64\n",
    "# remove 300 negative t2 scans and 100 negative T1post scans from the negative_set\n",
    "negative_set_t2 = negative_set[negative_set['Modality'] == 'MRI T2']\n",
    "negative_set_t1post = negative_set[negative_set['Modality'] == 'MRI T1post']\n",
    "\n",
    "negative_set_t2_ids = set(negative_set_t2['SeriesInstanceUID'])\n",
    "negative_set_t1post_ids = set(negative_set_t1post['SeriesInstanceUID'])\n",
    "\n",
    "to_remove_t2 = list(negative_set_t2_ids)[:300]\n",
    "to_remove_t1post = list(negative_set_t1post_ids)[:100]\n",
    "\n",
    "negative_set = negative_set[~negative_set['SeriesInstanceUID'].isin(to_remove_t2)]\n",
    "negative_set = negative_set[~negative_set['SeriesInstanceUID'].isin(to_remove_t1post)]\n",
    "\n",
    "negative_set_ids = set(negative_set['SeriesInstanceUID'])\n",
    "# print(negative_set['Modality'].value_counts())\n",
    "# Modality\n",
    "# CTA           860\n",
    "# MRA           622\n",
    "# MRI T2        322\n",
    "# MRI T1post    128\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9f9a2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_mean_and_std(normalized_mean, normalized_std, x, y):\n",
    "    rescaled_mean = x + normalized_mean * (y - x)\n",
    "    rescaled_std = normalized_std * (y - x)\n",
    "    return rescaled_mean, rescaled_std\n",
    "\n",
    "def process_single_series(series_instance_uid, series_root_path, output_path, mean, std_deviation, is_validation=False):\n",
    "    \"\"\"Process a single series and save files directly\"\"\"\n",
    "    series_folder_path = os.path.join(series_root_path, str(series_instance_uid))\n",
    "    if not os.path.exists(series_folder_path):\n",
    "        return None\n",
    "    \n",
    "    # Determine output folder\n",
    "    folder_name = 'validation' if is_validation else 'train'\n",
    "    output_folder = os.path.join(output_path, folder_name)\n",
    "    \n",
    "    # Get and sort DCM files\n",
    "    dcm_file_paths = [(os.path.join(series_folder_path, f), f) for f in os.listdir(series_folder_path) if f.endswith('.dcm')]\n",
    "    if not dcm_file_paths:\n",
    "        return None\n",
    "    \n",
    "    dcm_files = []\n",
    "    for dcm_file_path, file_name in dcm_file_paths:\n",
    "        dcm_data = pydicom.dcmread(dcm_file_path, stop_before_pixels=True)\n",
    "        dcm_files.append((dcm_file_path, dcm_data.InstanceNumber, file_name.rstrip(\".dcm\")))\n",
    "    dcm_files.sort(key=lambda x: x[1])\n",
    "    \n",
    "    results = []\n",
    "    max_instance = max([dcm[1] for dcm in dcm_files])\n",
    "    \n",
    "    # Create output series folder (unique per series - no conflicts)\n",
    "    output_series_folder = os.path.join(output_folder, str(series_instance_uid))\n",
    "    os.makedirs(output_series_folder, exist_ok=True)\n",
    "    \n",
    "    # Sample 1: Close to distribution\n",
    "    rescaled_mean, rescaled_std = rescale_mean_and_std(mean, std_deviation, 0, len(dcm_files))\n",
    "    sampled_dcm = np.random.normal(rescaled_mean, rescaled_std)\n",
    "    closest_dcm = min(dcm_files, key=lambda x: abs(x[1] - sampled_dcm))\n",
    "    \n",
    "    pixel_array = pydicom.dcmread(closest_dcm[0]).pixel_array\n",
    "    output_file_path = os.path.join(output_series_folder, f\"{closest_dcm[2]}.npy\")\n",
    "    np.save(output_file_path, pixel_array)\n",
    "    \n",
    "    results.append((series_instance_uid, closest_dcm[2], closest_dcm[1] / max_instance))\n",
    "    collected_instance_number = closest_dcm[1]\n",
    "\n",
    "    list_of_free_instance_numbers = [dcm[1] for dcm in dcm_files if dcm[1] != collected_instance_number]\n",
    "    # Sample 2: Random\n",
    "    if len(dcm_files) > 1:\n",
    "        selected_instance_number = np.random.choice(list_of_free_instance_numbers)\n",
    "        random_dcm = next(dcm for dcm in dcm_files if dcm[1] == selected_instance_number)\n",
    "        pixel_array = pydicom.dcmread(random_dcm[0]).pixel_array\n",
    "        output_file_path = os.path.join(output_series_folder, f\"{random_dcm[2]}.npy\")\n",
    "        np.save(output_file_path, pixel_array)\n",
    "        \n",
    "        results.append((series_instance_uid, random_dcm[2], random_dcm[1] / max_instance))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_dicom_files_negative_parallel(series_root_path, output_path, previous_samples, negative_set_ids, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Process negative DICOM files in parallel using joblib\n",
    "    n_jobs: number of parallel jobs (-1 uses all available cores)\n",
    "    \"\"\"\n",
    "    print(f\"Using joblib for parallel processing...\")\n",
    "    \n",
    "    # Create output folders\n",
    "    train_folder = os.path.join(output_path, 'train')\n",
    "    validation_folder = os.path.join(output_path, 'validation')\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(validation_folder, exist_ok=True)\n",
    "    \n",
    "    mean = np.mean(previous_samples)\n",
    "    std_deviation = np.std(previous_samples)\n",
    "    \n",
    "    # Pre-determine train/validation split\n",
    "    negative_set_list = list(negative_set_ids)\n",
    "    random.shuffle(negative_set_list)  # Shuffle to ensure random split\n",
    "    split_point = int(len(negative_set_list) * 0.8)\n",
    "    \n",
    "    print(f\"Processing {len(negative_set_list)} negative series...\")\n",
    "    print(f\"Train: {split_point}, Validation: {len(negative_set_list) - split_point}\")\n",
    "    \n",
    "    # Create tasks with train/val designation\n",
    "    tasks = []\n",
    "    for i, series_id in enumerate(negative_set_list):\n",
    "        is_validation = i >= split_point\n",
    "        tasks.append((series_id, series_root_path, output_path, mean, std_deviation, is_validation))\n",
    "    \n",
    "    # Process in parallel\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "        delayed(process_single_series)(series_id, series_root_path, output_path, mean, std_deviation, is_validation)\n",
    "        for series_id, series_root_path, output_path, mean, std_deviation, is_validation in tasks\n",
    "    )\n",
    "    \n",
    "    # Collect results\n",
    "    validation_train_split = {'val': [], 'train': []}\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        if result:\n",
    "            is_validation = i >= split_point\n",
    "            split_key = 'val' if is_validation else 'train'\n",
    "            validation_train_split[split_key].extend(result)\n",
    "    \n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"Train samples: {len(validation_train_split['train'])}\")\n",
    "    print(f\"Validation samples: {len(validation_train_split['val'])}\")\n",
    "    \n",
    "    return validation_train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1a7f8738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using joblib for parallel processing...\n",
      "Processing 1904 negative series...\n",
      "Train: 1523, Validation: 381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 11 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Train samples: 3046\n",
      "Validation samples: 762\n"
     ]
    }
   ],
   "source": [
    "validation_train_split = process_dicom_files_negative_parallel(str(SERIES_DIR), str(DATA_DIR), positions_sampled, negative_set_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1226d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = validation_train_split['train']\n",
    "val_array = validation_train_split['val']\n",
    "df_training_class_negative, _ = create_datafames_for(train_array, df_localizers, df_train, skip_loc=True)\n",
    "df_validation_class_negative, _ = create_datafames_for(val_array, df_localizers, df_train, skip_loc=True)\n",
    "# test for duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d5f4ef5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_training_class_negative.groupby(['SeriesInstanceUID', 'SOPInstanceUID']).count()) == len(df_training_class_negative)\n",
    "#len(df_validation_class_negative.groupby(['SeriesInstanceUID', 'SOPInstanceUID']).count()) == len(df_validation_class_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "553370b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training dataframe saved with 4691 rows\n"
     ]
    }
   ],
   "source": [
    "df_train_combined = pd.concat([df_training_class_positive, df_training_class_negative], \n",
    "                             ignore_index=True)\n",
    "df_train_combined.to_csv('train.csv', index=False)\n",
    "print(f\"Combined training dataframe saved with {len(df_train_combined)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "763b74ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined validation dataframe saved with 1186 rows\n"
     ]
    }
   ],
   "source": [
    "df_val_combined = pd.concat([df_validation_class_positive, df_validation_class_negative], \n",
    "                           ignore_index=True)\n",
    "\n",
    "df_val_combined.to_csv('validation.csv', index=False)\n",
    "\n",
    "print(f\"Combined validation dataframe saved with {len(df_val_combined)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "aaa80cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_labels_positive.to_csv('validation_labels.csv', index=False)\n",
    "df_training_labels_positive.to_csv('training_labels.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
